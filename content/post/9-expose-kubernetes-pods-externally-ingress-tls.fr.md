---
slug: expose-kubernetes-pods-externally-ingress-tls
title: Exposer des Pods Kubernetes en externe avec Ingress et TLS
description: D√©couvrez comment exposer des pods Kubernetes en externe avec Services, Ingress et TLS gr√¢ce √† BGP, NGINX et Cert-Manager dans un homelab.
date: 2025-08-19
draft: false
tags:
  - kubernetes
  - helm
  - bgp
  - opnsense
  - cilium
  - nginx-ingress-controller
  - cert-manager
categories:
  - homelab
---

## Intro

Apr√®s avoir construit mon propre cluster Kubernetes dans mon homelab avec `kubeadm` dans [cet article]({{< ref "post/8-create-manual-kubernetes-cluster-kubeadm" >}}), mon prochain d√©fi est d‚Äôexposer un pod simple √† l‚Äôext√©rieur, accessible via une URL et s√©curis√© avec un certificat TLS valid√© par Let‚Äôs Encrypt.

Pour y parvenir, j‚Äôai besoin de configurer plusieurs composants :
- **Service** : Expose le pod √† l‚Äôint√©rieur du cluster et fournit un point d‚Äôacc√®s.
- **Ingress** : D√©finit des r√®gles de routage pour exposer des services HTTP(S) √† l‚Äôext√©rieur.
- **Ingress Controller** : Surveille les ressources Ingress et g√®re r√©ellement le routage du trafic.
- **Certificats TLS** : S√©curisent le trafic en HTTPS gr√¢ce √† des certificats d√©livr√©s par Let‚Äôs Encrypt.

Cet article vous guide pas √† pas pour comprendre comment fonctionne l‚Äôacc√®s externe dans Kubernetes dans un environnement homelab.

C'est parti.

---
## Helm

J‚Äôutilise **Helm**, le gestionnaire de paquets de facto pour Kubernetes, afin d‚Äôinstaller des composants externes comme l‚ÄôIngress Controller ou cert-manager.

### Pourquoi Helm

Helm simplifie le d√©ploiement et la gestion des applications Kubernetes. Au lieu d‚Äô√©crire et de maintenir de longs manifestes YAML, Helm permet d‚Äôinstaller des applications en une seule commande, en s‚Äôappuyant sur des charts versionn√©s et configurables.

### Installer Helm

J‚Äôinstalle Helm sur mon h√¥te bastion LXC, qui dispose d√©j√† d‚Äôun acc√®s au cluster Kubernetes :
```bash
curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt update
sudo apt install helm
```

---
## Services Kubernetes

Avant de pouvoir exposer un pod √† l‚Äôext√©rieur, il faut d‚Äôabord le rendre accessible √† l‚Äôint√©rieur du cluster. C‚Äôest l√† qu‚Äôinterviennent les **Services Kubernetes**.

Les Services agissent comme un pont entre les pods et le r√©seau, garantissant que les applications restent accessibles m√™me si les pods sont r√©ordonn√©s ou red√©ploy√©s.

Il existe plusieurs types de Services Kubernetes, chacun avec un objectif diff√©rent :
- **ClusterIP** expose le Service sur une IP interne au cluster, uniquement accessible depuis l‚Äôint√©rieur.
- **NodePort** expose le Service sur un port statique de l‚ÄôIP de chaque n≈ìud, accessible depuis l‚Äôext√©rieur du cluster.
- **LoadBalancer** expose le Service sur une IP externe, g√©n√©ralement via une int√©gration cloud (ou via BGP dans un homelab).

---

## Exposer un Service `LoadBalancer` avec BGP

Au d√©part, j‚Äôai envisag√© d‚Äôutiliser **MetalLB** pour exposer les adresses IP des services sur mon r√©seau local. C‚Äôest ce que j‚Äôutilisais auparavant quand je d√©pendais de la box de mon FAI comme routeur principal. Mais apr√®s avoir lu cet article, [Use Cilium BGP integration with OPNsense](https://devopstales.github.io/kubernetes/cilium-opnsense-bgp/), je r√©alise que je peux obtenir le m√™me r√©sultat (voire mieux) en utilisant **BGP** avec mon routeur **OPNsense** et **Cilium**, mon CNI.

### Qu‚Äôest-ce que BGP ?

BGP (_Border Gateway Protocol_) est un protocole de routage utilis√© pour √©changer des routes entre syst√®mes. Dans un homelab Kubernetes, BGP permet √† tes n≈ìuds Kubernetes d‚Äôannoncer directement leurs IPs √† ton routeur ou firewall. Ton routeur sait alors exactement comment atteindre les adresses IP g√©r√©es par ton cluster.

Au lieu que MetalLB g√®re l‚Äôallocation d‚ÄôIP et les r√©ponses ARP, tes n≈ìuds disent directement √† ton routeur : ¬´ H√©, c‚Äôest moi qui poss√®de l‚Äôadresse 192.168.1.240 ¬ª.

### L‚Äôapproche MetalLB classique

Sans BGP, MetalLB en mode Layer 2 fonctionne comme ceci :
- Il assigne une adresse IP `LoadBalancer` (par exemple `192.168.1.240`) depuis un pool.
- Un n≈ìud r√©pond aux requ√™tes ARP pour cette IP sur ton LAN.

Oui, MetalLB peut aussi fonctionner avec BGP, mais pourquoi l‚Äôutiliser si mon CNI (Cilium) le g√®re d√©j√† nativement ?

### BGP avec Cilium

Avec Cilium + BGP, tu obtiens :
- L‚Äôagent Cilium du n≈ìud annonce les IPs `LoadBalancer` via BGP.
- Ton routeur apprend ces routes et les envoie au bon n≈ìud.
- Plus besoin de MetalLB.

### Configuration BGP

BGP est d√©sactiv√© par d√©faut, aussi bien sur OPNsense que sur Cilium. Activons-le des deux c√¥t√©s.

#### Sur OPNsense

D‚Äôapr√®s la [documentation officielle OPNsense](https://docs.opnsense.org/manual/dynamic_routing.html#bgp-section), l‚Äôactivation de BGP n√©cessite d‚Äôinstaller un plugin.

Va dans `System` > `Firmware` > `Plugins` et installe le plugin **os-frr** :  
![  ](img/opnsense-add-os-frr-plugin.png)
Installer le plugin `os-frr` dans OPNsense

Une fois install√©, active le plugin dans `Routing` > `General` :  
![  ](img/opnsense-enable-routing-frr-plugin.png)
Activer le routage dans OPNsense

Ensuite, rends-toi dans la section **BGP**. Dans l‚Äôonglet **General** :
- Coche la case pour activer BGP.
- D√©fini ton **ASN BGP**. J‚Äôai choisi `64512`, le premier ASN priv√© de la plage r√©serv√©e (voir [ASN table](https://en.wikipedia.org/wiki/Autonomous_system_\(Internet\)#ASN_Table)) :  
![  ](img/opnsense-enable-bgp.png)

Ajoute ensuite tes voisins BGP. Je ne fais le peering qu‚Äôavec mes **n≈ìuds workers** (puisque seuls eux h√©bergent des workloads). Pour chaque voisin :
- Mets l‚ÄôIP du n≈ìud dans `Peer-IP`.
- Utilise `64513` comme **Remote AS** (celui de Cilium).
- Configure `Update-Source Interface` sur `Lab`.
- Coche `Next-Hop-Self`.  
![  ](img/opnsense-bgp-create-neighbor.png)

Voici la liste de mes voisins une fois configur√©s :  
![  ](img/opnsense-bgp-neighbor-list.png)
Liste des voisins BGP

N‚Äôoublie pas la r√®gle firewall pour autoriser BGP (port `179/TCP`) depuis le VLAN **Lab** vers le firewall :  
![  ](img/opnsense-create-firewall-rule-bgp-peering.png)
Autoriser TCP/179 de Lab vers OPNsense

#### Dans Cilium

J‚Äôai d√©j√† Cilium install√© et je n‚Äôai pas trouv√© comment activer BGP avec la CLI, donc je l‚Äôai simplement r√©install√© avec l‚Äôoption BGP :

```bash
cilium uninstall
cilium install --set bgpControlPlane.enabled=true
```

Je configure uniquement les **n≈ìuds workers** pour √©tablir le peering BGP en les labellisant avec un `nodeSelector` :
```bash
kubectl label node apex-worker node-role.kubernetes.io/worker=""
kubectl label node vertex-worker node-role.kubernetes.io/worker=""
kubectl label node zenith-worker node-role.kubernetes.io/worker=""
```
```plaintext
NAME            STATUS   ROLES           AGE    VERSION
apex-master     Ready    control-plane   5d4h   v1.32.7
apex-worker     Ready    worker          5d1h   v1.32.7
vertex-master   Ready    control-plane   5d1h   v1.32.7
vertex-worker   Ready    worker          5d1h   v1.32.7
zenith-master   Ready    control-plane   5d1h   v1.32.7
zenith-worker   Ready    worker          5d1h   v1.32.7
```

Pour la configuration BGP compl√®te, j‚Äôai besoin de :
- **CiliumBGPClusterConfig** : param√®tres BGP pour le cluster Cilium, incluant son ASN local et son pair.
- **CiliumBGPPeerConfig** : d√©finit les timers, le red√©marrage gracieux et les routes annonc√©es.
- **CiliumBGPAdvertisement** : indique quels services Kubernetes annoncer via BGP.
- **CiliumLoadBalancerIPPool** : d√©finit la plage d‚ÄôIPs attribu√©es aux services `LoadBalancer`.

```yaml
---
apiVersion: cilium.io/v2alpha1
kind: CiliumBGPClusterConfig
metadata:
  name: bgp-cluster
spec:
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker: "" # Only for worker nodes
  bgpInstances:
  - name: "cilium-bgp-cluster"
    localASN: 64513 # Cilium ASN
    peers:
    - name: "pfSense-peer"
      peerASN: 64512 # OPNsense ASN
      peerAddress: 192.168.66.1  # OPNsense IP
      peerConfigRef:
        name: "bgp-peer"
---
apiVersion: cilium.io/v2alpha1
kind: CiliumBGPPeerConfig
metadata:
  name: bgp-peer
spec:
  timers:
    holdTimeSeconds: 9
    keepAliveTimeSeconds: 3
  gracefulRestart:
    enabled: true
    restartTimeSeconds: 15
  families:
    - afi: ipv4
      safi: unicast
      advertisements:
        matchLabels:
          advertise: "bgp"
---
apiVersion: cilium.io/v2alpha1
kind: CiliumBGPAdvertisement
metadata:
  name: bgp-advertisement
  labels:
    advertise: bgp
spec:
  advertisements:
    - advertisementType: "Service"
      service:
        addresses:
          - LoadBalancerIP
      selector:
        matchExpressions:
          - { key: somekey, operator: NotIn, values: [ never-used-value ] }
---
apiVersion: "cilium.io/v2alpha1"
kind: CiliumLoadBalancerIPPool
metadata:
  name: "dmz"
spec:
  blocks:
  - start: "192.168.55.20" # LB Range Start IP
    stop: "192.168.55.250" # LB Range End IP
```

Applique la configuration :
```bash
kubectl apply -f bgp.yaml 

ciliumbgpclusterconfig.cilium.io/bgp-cluster created
ciliumbgppeerconfig.cilium.io/bgp-peer created
ciliumbgpadvertisement.cilium.io/bgp-advertisement created
ciliumloadbalancerippool.cilium.io/dmz created
```

Si tout fonctionne, tu devrais voir les sessions BGP **√©tablies** avec tes workers :
```bash
cilium bgp peers

Node            Local AS   Peer AS   Peer Address   Session State   Uptime   Family         Received   Advertised
apex-worker     64513      64512     192.168.66.1   established     6m30s    ipv4/unicast   1          2    
vertex-worker   64513      64512     192.168.66.1   established     7m9s     ipv4/unicast   1          2    
zenith-worker   64513      64512     192.168.66.1   established     6m13s    ipv4/unicast   1          2
```

### D√©ployer un Service `LoadBalancer` avec BGP

Validons rapidement que la configuration fonctionne en d√©ployant un `Deployment` de test et un `Service` de type `LoadBalancer` :
```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: test-lb
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
    name: http
  selector:
    svc: test-lb
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  selector:
    matchLabels:
      svc: test-lb
  template:
    metadata:
      labels:
        svc: test-lb
    spec:
      containers:
      - name: web
        image: nginx
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /
            port: 80
```

V√©rifions si le service obtient une IP externe :
```bash
kubectl get services test-lb

NAME         TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)        AGE
test-lb      LoadBalancer   10.100.167.198   192.168.55.20   80:31350/TCP   169m
```

Le service a r√©cup√©r√© la premi√®re IP du pool d√©fini : `192.168.55.20`.

Depuis n‚Äôimporte quel appareil du LAN, on peut tester l‚Äôacc√®s sur le port 80 :  
![Test LoadBalancer service with BGP](img/k8s-test-loadbalancer-service-with-bgp.png)

‚úÖ Notre pod est joignable via une IP `LoadBalancer` rout√©e en BGP. Premi√®re √©tape r√©ussie !

---
## Kubernetes Ingress

Nous avons r√©ussi √† exposer un pod en externe en utilisant un service `LoadBalancer` et une adresse IP attribu√©e via BGP. Cette approche fonctionne tr√®s bien pour les tests, mais elle ne fonctionne pas √† l‚Äô√©chelle.

Imagine avoir 10, 20 ou 50 services diff√©rents. Est-ce que je voudrais vraiment allouer 50 adresses IP et encombrer mon firewall ainsi que mes tables de routage avec 50 entr√©es BGP ? Certainement pas.

C‚Äôest l√† qu‚Äôintervient **Ingress**.

### Qu‚Äôest-ce qu‚Äôun Kubernetes Ingress ?

Un Kubernetes **Ingress** est un objet API qui g√®re **l‚Äôacc√®s externe aux services** d‚Äôun cluster, g√©n√©ralement en HTTP et HTTPS, le tout via un point d‚Äôentr√©e unique.

Au lieu d‚Äôattribuer une IP par service, on d√©finit des r√®gles de routage bas√©es sur :
- **Des noms d‚Äôh√¥tes** (`app1.vezpi.me`, `blog.vezpi.me`, etc.)
- **Des chemins** (`/grafana`, `/metrics`, etc.)
    

Avec Ingress, je peux exposer plusieurs services via la m√™me IP et le m√™me port (souvent 443 pour HTTPS), et Kubernetes saura comment router la requ√™te vers le bon service backend.

Voici un exemple simple d‚Äô`Ingress`, qui route le trafic de `test.vezpi.me` vers le service `test-lb` sur le port 80 :
```yaml
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
spec:
  rules:
    - host: test.vezpi.me
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: test-lb
                port:
                  number: 80
```

### Ingress Controller

Un Ingress, en soi, n‚Äôest qu‚Äôun ensemble de r√®gles de routage. Il ne traite pas r√©ellement le trafic. Pour le rendre fonctionnel, il faut un **Ingress Controller**, qui va :
- Surveiller l‚ÄôAPI Kubernetes pour d√©tecter les ressources `Ingress`.
- Ouvrir les ports HTTP(S) via un service `LoadBalancer` ou `NodePort`.
- Router le trafic vers le bon `Service` selon les r√®gles de l‚ÄôIngress.

Parmi les contr√¥leurs populaires, on retrouve NGINX, Traefik, HAProxy, et d‚Äôautres encore. Comme je cherchais quelque chose de simple, stable et largement adopt√©, j‚Äôai choisi le **NGINX Ingress Controller**.

### Installer NGINX Ingress Controller

J‚Äôutilise Helm pour installer le contr√¥leur, et je d√©finis `controller.ingressClassResource.default=true` pour que tous mes futurs ingress l‚Äôutilisent par d√©faut :
```bash
helm install ingress-nginx \
  --repo=https://kubernetes.github.io/ingress-nginx \
  --namespace=ingress-nginx \
  --create-namespace ingress-nginx \
  --set controller.ingressClassResource.default=true \
  --set controller.config.strict-validate-path-type=false
```

Le contr√¥leur est d√©ploy√© et expose un service `LoadBalancer`. Dans mon cas, il r√©cup√®re la deuxi√®me adresse IP disponible dans la plage BGP :
```bash
NAME                       TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                      AGE   SELECTOR
ingress-nginx-controller   LoadBalancer   10.106.236.13   192.168.55.21   80:31195/TCP,443:30974/TCP   75s   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx
```

### R√©server une IP statique pour le contr√¥leur

Je veux m‚Äôassurer que l‚ÄôIngress Controller re√ßoive toujours la m√™me adresse IP. Pour cela, j‚Äôai cr√©√© deux pools d‚ÄôIP Cilium distincts :
- Un r√©serv√© pour l‚ÄôIngress Controller avec une seule IP.
- Un pour tout le reste.
```yaml
---
# Pool for Ingress Controller
apiVersion: cilium.io/v2alpha1
kind: CiliumLoadBalancerIPPool
metadata:
  name: ingress-nginx
spec:
  blocks:
    - cidr: 192.168.55.55/32
  serviceSelector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/component: controller
---
# Default pool for other services
apiVersion: cilium.io/v2alpha1
kind: CiliumLoadBalancerIPPool
metadata:
  name: default
spec:
  blocks:
    - start: 192.168.55.100
      stop: 192.168.55.250
  serviceSelector:
    matchExpressions:
      - key: app.kubernetes.io/name
        operator: NotIn
        values:
          - ingress-nginx
```

Apr√®s avoir remplac√© le pool partag√© par ces deux pools, l‚ÄôIngress Controller re√ßoit bien l‚ÄôIP d√©di√©e `192.168.55.55`, et le service `test-lb` obtient `192.168.55.100` comme pr√©vu :
```bash
NAMESPACE       NAME                                 TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)                      AGE
default         test-lb                              LoadBalancer   10.100.167.198   192.168.55.100   80:31350/TCP                 6h34m
ingress-nginx   ingress-nginx-controller             LoadBalancer   10.106.236.13    192.168.55.55    80:31195/TCP,443:30974/TCP   24m
```
### Associer un Service √† un Ingress

Maintenant, connectons un service √† ce contr√¥leur.

Je commence par mettre √† jour le service `LoadBalancer` d‚Äôorigine pour le convertir en `ClusterIP` (puisque c‚Äôest d√©sormais l‚ÄôIngress Controller qui l‚Äôexposera en externe) :
```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: test-lb
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    svc: test-lb
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
spec:
  rules:
    - host: test.vezpi.me
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: test-lb
                port:
                  number: 80  
```

Ensuite, j‚Äôapplique le manifeste `Ingress` pour exposer le service en HTTP.

Comme j‚Äôutilise le plugin **Caddy** dans OPNsense, j‚Äôai encore besoin d‚Äôun routage local de type Layer 4 pour rediriger le trafic de `test.vezpi.me` vers l‚Äôadresse IP de l‚ÄôIngress Controller (`192.168.55.55`). Je cr√©e donc une nouvelle r√®gle dans le plugin Caddy.

![Create Layer4 router in Caddy plugin for OPNsense](img/opnsense-caddy-create-layer4-route-http.png)

Puis je teste l‚Äôacc√®s dans le navigateur :  
![  ](img/ingress-controller-nginx-test-simple-webserver.png)
Test d‚Äôun Ingress en HTTP

‚úÖ Mon pod est d√©sormais accessible via son URL HTTP en utilisant un Ingress. Deuxi√®me √©tape compl√©t√©e !

---
## Connexion s√©curis√©e avec TLS

Exposer des services en HTTP simple est suffisant pour des tests, mais en pratique nous voulons presque toujours utiliser **HTTPS**. Les certificats TLS chiffrent le trafic et garantissent l‚Äôauthenticit√© ainsi que la confiance pour les utilisateurs.

### Cert-Manager

Pour automatiser la gestion des certificats dans Kubernetes, nous utilisons **Cert-Manager**. Il peut demander, renouveler et g√©rer les certificats TLS sans intervention manuelle.

#### Installer Cert-Manager

Nous le d√©ployons avec Helm dans le cluster :
```bash
helm repo add jetstack https://charts.jetstack.io
helm repo update
helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --set crds.enabled=true
```

#### Configurer Cert-Manager

Ensuite, nous configurons un **ClusterIssuer** pour Let‚Äôs Encrypt. Cette ressource indique √† Cert-Manager comment demander des certificats :
```yaml
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-staging
spec:
  acme:
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    email: <email>
    privateKeySecretRef:
      name: letsencrypt-staging-key
    solvers:
    - http01:
        ingress:
          ingressClassName: nginx
```

‚ÑπÔ∏è Ici, je d√©finis le serveur **staging** de Let‚Äôs Encrypt ACME pour les tests. Les certificats de staging ne sont pas reconnus par les navigateurs, mais ils √©vitent d‚Äôatteindre les limites strictes de Let‚Äôs Encrypt lors du d√©veloppement.

Appliquez-le :
```bash
kubectl apply -f clusterissuer.yaml
```

V√©rifiez si votre `ClusterIssuer` est `Ready` :
```bash
kubectl get clusterissuers.cert-manager.io                                                    
NAME                  READY   AGE
letsencrypt-staging   True    14m
```

S‚Äôil ne devient pas `Ready`, utilisez `kubectl describe` sur la ressource pour le diagnostiquer.

### Ajouter TLS dans un Ingress

Nous pouvons maintenant s√©curiser notre service avec TLS en ajoutant une section `tls` dans la sp√©cification `Ingress` et en r√©f√©ren√ßant le `ClusterIssuer` :
```yaml
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress-https
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: letsencrypt-staging
spec:
  tls:
    - hosts:
      - test.vezpi.me
      secretName: test-vezpi-me-tls
  rules:
    - host: test.vezpi.me
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: test-lb
                port:
                  number: 80
```

En arri√®re-plan, Cert-Manager suit ce flux pour √©mettre le certificat :
- D√©tecte l‚Äô`Ingress` avec `tls` et le `ClusterIssuer`.
- Cr√©e un CRD **Certificate** d√©crivant le certificat souhait√© + l‚Äôemplacement du Secret.
- Cr√©e un CRD **Order** pour repr√©senter une tentative d‚Äô√©mission avec Let‚Äôs Encrypt.
- Cr√©e un CRD **Challenge** (par ex. validation HTTP-01).
- Met en place un Ingress/Pod temporaire pour r√©soudre le challenge.
- Cr√©e un CRD **CertificateRequest** et envoie le CSR √† Let‚Äôs Encrypt.
- Re√ßoit le certificat sign√© et le stocke dans un Secret Kubernetes.
- L‚ÄôIngress utilise automatiquement ce Secret pour servir en HTTPS.

‚úÖ Une fois ce processus termin√©, votre Ingress est s√©curis√© avec un certificat TLS.  
![Certificat TLS valid√© avec le serveur de staging de Let‚Äôs Encrypt](img/k8s-test-deploy-service-tls-certificate-staging-lets-encrypt.png)

### Passer aux certificats de production

Une fois que le staging fonctionne, nous pouvons passer au serveur **production** ACME pour obtenir un certificat Let‚Äôs Encrypt reconnu :
```yaml
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: <email>
    privateKeySecretRef:
      name: letsencrypt-key
    solvers:
    - http01:
        ingress:
          ingressClassName: nginx
```

Mettez √† jour l‚Äô`Ingress` pour pointer vers le nouveau `ClusterIssuer` :
```yaml
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress-https
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
spec:
  tls:
    - hosts:
      - test.vezpi.me
      secretName: test-vezpi-me-tls
  rules:
    - host: test.vezpi.me
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: test-lb
                port:
                  number: 80
```

Comme le certificat de staging est encore stock√© dans le Secret, je le supprime pour forcer une nouvelle demande en production :
```bash
kubectl delete secret test-vezpi-me-tls
```

üéâ Mon `Ingress` est d√©sormais s√©curis√© avec un certificat TLS valide d√©livr√© par Let‚Äôs Encrypt. Les requ√™tes vers `https://test.vezpi.me` sont chiffr√©es de bout en bout et rout√©es par le NGINX Ingress Controller jusqu‚Äô√† mon pod `nginx` :  
![Ingress HTTPS avec certificat valid√© par Let‚Äôs Encrypt](img/k8s-deploy-test-service-tls-certificate-lets-encrypt.png)


---
## Conclusion

Dans ce parcours, je suis parti des bases, en exposant un simple pod avec un service `LoadBalancer`, puis j‚Äôai construit √©tape par √©tape une configuration pr√™te pour la production :
- Compr√©hension des **Services Kubernetes** et de leurs diff√©rents types.
- Utilisation du **BGP avec Cilium** et OPNsense pour attribuer des IP externes directement depuis mon r√©seau.
- Introduction des **Ingress** pour mieux passer √† l‚Äô√©chelle, en exposant plusieurs services via un point d‚Äôentr√©e unique.
- Installation du **NGINX Ingress Controller** pour g√©rer le routage.
- Automatisation de la gestion des certificats avec **Cert-Manager**, afin de s√©curiser mes services avec des certificats TLS Let‚Äôs Encrypt.

üöÄ R√©sultat : mon pod est maintenant accessible via une v√©ritable URL, s√©curis√© en HTTPS, comme n‚Äôimporte quelle application web moderne.

C‚Äôest une √©tape importante dans mon aventure Kubernetes en homelab. Dans le prochain article, je souhaite explorer le stockage persistant et connecter mon cluster Kubernetes √† mon setup **Ceph** sous **Proxmox**.

A la prochaine !